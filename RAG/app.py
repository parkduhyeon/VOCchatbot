import os
from dotenv import load_dotenv
from openai import AzureOpenAI
import streamlit as st


load_dotenv()

# Get environment variables
openai_endpoint = os.getenv("OPENAI_ENDPOINT")
openai_api_key = os.getenv("OPENAI_API_KEY")
chat_model = os.getenv("CHAT_MODEL")
embedding_model = os.getenv("EMBEDDING_MODEL")
search_endpoint=os.getenv("SEARCH_ENDPOINT")
search_api_key=os.getenv("SEARCH_API_KEY")
index_name = os.getenv("INDEX_NAME")

# Initialize Azure OpenAI client
chat_client = AzureOpenAI(
    api_version="2024-12-01-preview",
    azure_endpoint=openai_endpoint,
    api_key=openai_api_key
)

st.title("AICC VOC ì‘ëŒ€ ë´‡")
st.write("AICC ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ë¬¸ì˜ì‚¬í•­ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")

if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system",
            "content": "ë„ˆëŠ” AICCë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•˜ê³  ìˆëŠ” ê´€ë¦¬ìì•¼. 'dev-embedding-3-small' ëª¨ë¸ì—ì„œ AICC ì„œë¹„ìŠ¤ì˜ VOCë“¤ì„ RAGë¥¼ í†µí•´ì„œ ë³´ê´€í•˜ê³  ìˆëŠ”ë°, ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´ ì´ RAGë‚´ì˜ ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì²´í¬í•´ì„œ ë‹µë³€í•´ì¤˜. ë§Œì•½ ì´ RAGë‚´ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‹µë³€ì´ ì—†ë‹¤ë©´, 'í•´ë‹¹ ë‚´ìš©ì˜ VOCëŠ” ì¡°íšŒë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚´ìš©ì„ ì¡°íšŒí•´ ì£¼ì„¸ìš”.' ë¼ê³  ë‹µí•´ì¤˜."
        },
    ]

# Display chat history
for message in st.session_state.messages:
    if message["role"] != "system":  # ğŸ‘ˆ system ë©”ì‹œì§€ëŠ” UIì— ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        st.chat_message(message["role"]).write(message["content"])
    # st.chat_message(message["role"]).write(message["content"])



def get_openai_response(messages):

    # Additional parameters to apply RAG pattern using the AI Search index
    rag_params = {
        "data_sources": [
            {
                # he following params are used to search the index
                "type": "azure_search",
                "parameters": {
                    "endpoint": search_endpoint,
                    "index_name": index_name,
                    "authentication": {
                        "type": "api_key",
                        "key": search_api_key,
                    },
                    # The following params are used to vectorize the query
                    "query_type": "vector",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": embedding_model,
                    },
                }
            }
        ],
    }
        
    # Submit the chat request with RAG parameters
    response = chat_client.chat.completions.create(
        model=chat_model,
        messages=messages,
        extra_body=rag_params
    )

    completion = response.choices[0].message.content
    return completion

# Handle user input
if user_input := st.chat_input("Enter your question: "):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    with st.spinner("ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."):
        response = get_openai_response(st.session_state.messages)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)
    